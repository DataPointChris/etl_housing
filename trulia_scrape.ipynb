{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIES = None\n",
    "SAVE_DIRECTORY = 'daily_scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityScraper:\n",
    "    \"\"\"Scrapes a city for apartment listings.\"\"\"\n",
    "    \n",
    "    def __init__(self, city, save_directory='scraped'):\n",
    "        self.city = city\n",
    "        self.city_url = f'https://www.trulia.com/for_rent/{city}'\n",
    "        self.browser = self._set_browser()\n",
    "\n",
    "    def _set_browser(self):\n",
    "        firefox_options = webdriver.FirefoxOptions()\n",
    "        firefox_options.set_headless()\n",
    "        browser = webdriver.Firefox(firefox_options=firefox_options)\n",
    "        return browser\n",
    "\n",
    "    def browser_get(self, url):\n",
    "        \"\"\" Gets the url specified.  Contains error handling\"\"\"\n",
    "        try:\n",
    "            self.browser.get(url)\n",
    "        except (ConnectionError, ConnectionResetError) as e:\n",
    "            logger.info(f'Error {e} for URL: {url}')\n",
    "\n",
    "    def get_list_page_urls(self):\n",
    "        \"\"\"Gets the urls on the list page\"\"\"\n",
    "        listing_urls = []\n",
    "        for elem in self.browser.find_elements_by_class_name('jLNYlr'):\n",
    "            try:\n",
    "                listing_urls.append(elem.find_element_by_tag_name('a').get_attribute('href'))\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        return listing_urls\n",
    "\n",
    "    def get_next_page(self):\n",
    "        \"\"\" Gets next page link and returns flag indicating if last page\"\"\"\n",
    "        try:\n",
    "            # next_page = self.browser.find_element_by_class_name('bpESQu').get_attribute('href')\n",
    "            soup = BeautifulSoup(self.browser.page_source, 'html.parser')\n",
    "            href_suffix = soup.find('a', {'aria-label': 'Next Page'})['href']\n",
    "            next_page = 'https://www.trulia.com' + href_suffix\n",
    "            last_page = False\n",
    "            print(f'HREF SUFFIX: {href_suffix}')\n",
    "            return next_page, last_page\n",
    "        except TypeError:\n",
    "            next_page = None\n",
    "            last_page = True\n",
    "            return next_page, last_page\n",
    "\n",
    "    def get_apartment_urls_for_city(self):\n",
    "        '''Gets a list of urls for city from all listing pages'''\n",
    "        i = 1\n",
    "        url_list = []\n",
    "        next_page = self.city_url\n",
    "        last_page = False\n",
    "        while last_page is False:\n",
    "        # for _ in range(1):\n",
    "            print(f'Page {i}, Total Apartment URLs: {len(url_list)}') if i % 10 == 0 else None\n",
    "            self.browser_get(next_page)\n",
    "\n",
    "            list_page_urls = self.get_list_page_urls()\n",
    "            url_list.extend(list_page_urls)\n",
    "            print(f'Current URL: {self.browser.current_url}')\n",
    "            print(f'Last page: {last_page}')\n",
    "            next_page, last_page = self.get_next_page()\n",
    "            print(f'NEW Next page: {next_page}')\n",
    "            print(f'NEW Last page: {last_page}')\n",
    "            print(f'Iteration: {i}')\n",
    "            i += 1\n",
    "            time.sleep(.05)\n",
    "        return url_list\n",
    "\n",
    "    def get_apartment_data(self, url):\n",
    "        '''Gets apartment data for the url specified'''\n",
    "        self.browser_get(url)\n",
    "        content = self.browser.page_source\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        apartment_list = []\n",
    "\n",
    "        for floor_plan_table in soup.find_all('table', {'data-testid': 'floor-plan-group'}):\n",
    "            for tr in floor_plan_table.find_all('tr'):\n",
    "\n",
    "                unit = tr.find('div', {'color': 'highlight'}).text\n",
    "\n",
    "                sqft = tr.find('td', {'class': 'FloorPlanTable__FloorPlanFloorSpaceCell-sc-1ghu3y7-5'}).text\n",
    "\n",
    "                bed = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[0].text\n",
    "\n",
    "                bath = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[1].text\n",
    "\n",
    "                price = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanCell-sc-1ghu3y7-2',\n",
    "                                        'class': 'FloorPlanTable__FloorPlanSMCell-sc-1ghu3y7-8'},\n",
    "                                    limit=2)[1].text\n",
    "\n",
    "                name = soup.find('span', {'data-testid': 'home-details-summary-headline'}).text\n",
    "\n",
    "                address = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[0].text\n",
    "\n",
    "                city_state_zip = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[1].text\n",
    "\n",
    "                city, state, zipcode = city_state_zip.replace(',', '').rsplit(maxsplit=2)\n",
    "\n",
    "                description = soup.find('div', {'data-testid': 'home-description-text-description-text'}).text\n",
    "\n",
    "                details = [detail.text for detail in soup.find_all(\n",
    "                    'li', {'class': 'FeatureList__FeatureListItem-iipbki-0'}\n",
    "                )]\n",
    "                details = ' ,'.join(details)\n",
    "\n",
    "                apartment_url = url\n",
    "                date = str(datetime.datetime.now().date())\n",
    "                apartment_list.append([name, address, unit, sqft, bed, bath, price, city,\n",
    "                                    state, zipcode, description, details, apartment_url, date])\n",
    "        return apartment_list\n",
    "\n",
    "    def create_apartment_df(self, data):\n",
    "        df = pd.DataFrame(data, columns=['name', 'address', 'unit', 'sqft', 'bed', 'bath', 'price',\n",
    "                               'city', 'state', 'zipcode', 'description', 'details', 'url', 'date'])\n",
    "        return df\n",
    "\n",
    "    def clean_apartment_df(self, df):\n",
    "        '''Formats the dataframe to remove special characters, spaces, and NaN values.\n",
    "        Removes units with price ranges, rather than one specified price.'''\n",
    "\n",
    "        df.sqft = df.sqft.str.replace('sqft', '').str.replace(',', '').str.strip()\n",
    "        df = df.loc[df.sqft != ''] # throw out empty square foot apts\n",
    "        mask = df.sqft.str.contains('-')\n",
    "        df.loc[mask, 'sqft'] = df.loc[mask, 'sqft'].apply(lambda x: np.mean(list(map(int, x.split('-')))))\n",
    "        df.price = df.price.str.replace('Contact', '')\n",
    "        df.price = df.price.str.replace('$', '').str.replace(',', '').str.replace('+', '').str.strip()\n",
    "        df.bath = df.bath.str.replace('ba', '').str.strip()\n",
    "        df.bed = df.bed.str.replace('bd', '').str.lower().replace('studio', 0).str.strip()\n",
    "        df.bed = df.bed.replace(np.nan, 0)\n",
    "        df = df[~df.price.str.contains('-', na=False)]  # throw out price range apts\n",
    "        df.replace(' ', '', inplace=True)  # whitespace to blank\n",
    "        df.replace('', np.nan, inplace=True)  # blank to NaN\n",
    "        # df.dropna(inplace=True)  # drop NaN rows\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def convert_df_columns(self, df):\n",
    "        '''Converts rows to numeric and float for calculations'''\n",
    "        df = df.astype({'sqft': 'int32', 'price': 'int32', 'bath': 'float32', \n",
    "                        'bed': 'float32', 'zipcode': 'int32'})\n",
    "        return df\n",
    "\n",
    "    def save_to_csv(self, df):\n",
    "        '''Saves cleaned dataframe to csv file in \"daily_scrape_files\" folder\"'''\n",
    "        scraped_date = str(datetime.datetime.now().date())\n",
    "        city_name = self.city.replace(',', '_')\n",
    "        filedir = f'{SAVE_DIRECTORY}/{city_name}'\n",
    "        os.makedirs(filedir, exist_ok=True)\n",
    "        df.to_csv(f'{filedir}/{scraped_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START scraping Woburn,MA\n",
      "Getting URL list\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/\n",
      "Last page: False\n",
      "HREF SUFFIX: /for_rent/Woburn,MA/2_p/\n",
      "NEW Next page: https://www.trulia.com/for_rent/Woburn,MA/2_p/\n",
      "NEW Last page: False\n",
      "Iteration: 1\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/2_p/\n",
      "Last page: False\n",
      "HREF SUFFIX: /for_rent/Woburn,MA/3_p/\n",
      "NEW Next page: https://www.trulia.com/for_rent/Woburn,MA/3_p/\n",
      "NEW Last page: False\n",
      "Iteration: 2\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/3_p/\n",
      "Last page: False\n",
      "NEW Next page: None\n",
      "NEW Last page: True\n",
      "Iteration: 3\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]URLs retrieved: 92\n",
      "Getting apartment data from url_list\n",
      "100%|██████████| 92/92 [01:37<00:00,  1.06s/it]Apartments retrieved: 175\n",
      "FINISH scraping Woburn,MA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities = ['Woburn,MA']\n",
    "\n",
    "for city in cities:\n",
    "    print(f'START scraping {city}')    \n",
    "    scraper = CityScraper(city, save_directory=SAVE_DIRECTORY)\n",
    "    \n",
    "    print('Getting URL list')\n",
    "    apartment_url_list = scraper.get_apartment_urls_for_city()\n",
    "    print(f'URLs retrieved: {len(apartment_url_list)}')\n",
    "    \n",
    "    print('Getting apartment data from url_list')\n",
    "    apartments_data = []\n",
    "    for i, apartment_url in enumerate(tqdm(apartment_url_list), start=1):\n",
    "        print(f'URL {i} of {len(apartment_url_list)}') if i % 500 == 0 else None\n",
    "        try:\n",
    "            apt_data = scraper.get_apartment_data(apartment_url)\n",
    "            apartments_data.extend(apt_data)\n",
    "        except Exception as e:\n",
    "            print(f'Exception getting apartment data for url {apartment_url}', e)\n",
    "            # logger.exception('Error adding data to list', e)\n",
    "            continue\n",
    "    print(f'Apartments retrieved: {len(apartments_data)}')\n",
    "    \n",
    "    apartment_df = scraper.create_apartment_df(apartments_data)\n",
    "    cleaned_apartment_df = scraper.clean_apartment_df(apartment_df)\n",
    "    cleaned_and_converted_apartment_df = scraper.convert_df_columns(cleaned_apartment_df)\n",
    "    scraper.save_to_csv(cleaned_and_converted_apartment_df)\n",
    "    print(f'FINISH scraping {city}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}