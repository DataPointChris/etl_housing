{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to current working directory for cron job\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s:%(name)s:%(message)s')\n",
    "file_handler = logging.FileHandler('scraper.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "               'accept-encoding': 'gzip, deflate, sdch, br',\n",
    "               'accept-language': 'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
    "               'cache-control': 'max-age=0',\n",
    "               'upgrade-insecure-requests': '1',\n",
    "               'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}\n",
    "\n",
    "base_url = 'https://www.trulia.com'\n",
    "page_url = '/for_rent/Austin,TX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list(base_url, page_url):\n",
    "    '''Gets a list of urls from main page to scrape.'''\n",
    "    \n",
    "    start_time = time.time()\n",
    "    url_list = []\n",
    "    last_page = False\n",
    "    logger.info('Getting url_list')\n",
    "    while (last_page == False):\n",
    "        \n",
    "        response = requests.get(base_url+page_url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.info(\"Failed: \", response.status_code)\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        container = soup.find('div', {'data-testid': 'search-result-list-container'})\n",
    "\n",
    "        for div in soup.find_all('div', {'data-hero-element-id': 'srp-home-card', 'data-hero-element-id':'false'}):\n",
    "\n",
    "            url = div.find('a').attrs['href']\n",
    "            url_list.append(url)\n",
    "\n",
    "        ## Check if last page and exit while loop\n",
    "            \n",
    "        if soup.find('a', {'aria-label': 'Next Page'}):\n",
    "            last_page = False\n",
    "            page_url = soup.find('a', {'aria-label': 'Next Page'})['href']\n",
    "            sleep(.5)\n",
    "        else:\n",
    "\n",
    "            last_page = True\n",
    "            logger.info(f'Done getting url_list, length of {len(url_list)}')\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logger.info(f'Elapsed time: {round(elapsed_time/60,2)} minutes')\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "\n",
    "def get_apartment_data(base_url, current_url):\n",
    "    '''Gets apartment data for the url specified'''\n",
    "    \n",
    "    response = requests.get(base_url+current_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        logger.info(f'Failed: {response.status_code}')\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    apartment_list = []\n",
    "\n",
    "    for floor_plan_table in soup.find_all('table', {'data-testid': 'floor-plan-group'}):\n",
    "        for tr in floor_plan_table.find_all('tr'):\n",
    "\n",
    "            unit = tr.find('div', {'color': 'highlight'}).text\n",
    "\n",
    "            sqft = tr.find('td', {'class': 'FloorPlanTable__FloorPlanFloorSpaceCell-sc-1ghu3y7-5'}).text\n",
    "            \n",
    "            bed = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[0].text\n",
    "            \n",
    "            bath = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[1].text\n",
    "\n",
    "            price = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanCell-sc-1ghu3y7-2', \n",
    "                                       'class': 'FloorPlanTable__FloorPlanSMCell-sc-1ghu3y7-8'},\n",
    "                                        limit=2)[1].text\n",
    "\n",
    "            name = soup.find('span', {'data-testid': 'home-details-summary-headline'}).text\n",
    "\n",
    "            address = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[0].text\n",
    "\n",
    "            city_state_zip = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[1].text\n",
    "            \n",
    "            city, state, zipcode = city_state_zip.replace(',','').rsplit(maxsplit=2)\n",
    "            \n",
    "            description = soup.find('div', {'data-testid': 'home-description-text-description-text'}).text\n",
    "\n",
    "            details = [detail.text for detail in soup.find_all(\n",
    "                                                        'li', {'class': 'FeatureList__FeatureListItem-iipbki-0 dArMue'}\n",
    "                                                            )]\n",
    "            details = ' ,'.join(details)\n",
    "\n",
    "            apartment_url = base_url + current_url\n",
    "            \n",
    "            date = str(datetime.datetime.now().date())\n",
    "\n",
    "            apartment_list.append([name, address, unit, sqft, bed, bath, price, city, state, zipcode, description, details, apartment_url, date])\n",
    "    \n",
    "    return apartment_list\n",
    "\n",
    "\n",
    "\n",
    "def get_all_apartments(url_list):\n",
    "    '''Wrapper function using \"get_apartment_data\" function to get data for all apartments in \"url_list\"'''\n",
    "    apts_data = []\n",
    "    start_time = time.time()\n",
    "    logger.info(f'Getting apartment data from url_list')\n",
    "    i=1\n",
    "    for current_url in url_list:\n",
    "        if i % 500 == 0:\n",
    "            logger.info(f'URL {i} of {len(url_list)}')\n",
    "        i += 1\n",
    "        sleep(.05)\n",
    "        try:\n",
    "            apts_data.extend(get_apartment_data(base_url, current_url))\n",
    "        except Exception:\n",
    "            logger.exception('Error adding data to list')\n",
    "            continue\n",
    "\n",
    "    logger.info(f'Finished getting apartment data. Total apartments: {len(apts_data)}')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f'Elapsed time: {round(elapsed_time/60,2)} minutes')\n",
    "    \n",
    "    return apts_data\n",
    "\n",
    "\n",
    "\n",
    "def create_df(data):\n",
    "    logger.info('Creating DataFrame from apartment list.')\n",
    "    df = pd.DataFrame(data,\n",
    "                     columns=['name', 'address', 'unit', 'sqft', 'bed', 'bath', 'price', \n",
    "                              'city', 'state', 'zipcode', 'description', 'details', 'url', 'date'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def df_formatter(df):\n",
    "    '''Formats the dataframe to remove special characters, spaces, and NaN values.\n",
    "       Removes units with price ranges, rather than one specified price.\n",
    "       Converts rows to numeric and float for calculations'''\n",
    "    df.sqft = df.sqft.str.replace('sqft','').str.replace(',','').str.strip()\n",
    "\n",
    "    df.price = df.price.str.replace('Contact', '')\n",
    "\n",
    "    df.price = df.price.str.replace('$','').str.replace(',','').str.strip()\n",
    "\n",
    "    df.bath = df.bath.str.replace('ba','').str.strip()\n",
    "\n",
    "    df.bed = df.bed.str.replace('bd','').str.lower().replace('studio',0).str.strip()\n",
    "\n",
    "    df.bed = df.bed.replace(np.nan, 0)\n",
    "\n",
    "    df = df[~df.price.str.contains('-', na=False)]\n",
    "\n",
    "    df.replace(' ', '', inplace=True) # whitespace to blank\n",
    "    df.replace('', np.nan, inplace=True) # blank to NaN\n",
    "    df.dropna(inplace=True) # drop NaN rows\n",
    "\n",
    "    df = df.astype({'sqft': 'int32', 'price': 'int32', 'bath': 'float32', 'bed': 'float32', })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_to_csv():\n",
    "    '''Saves cleaned dataframe to csv file in \"daily_scrape_files\" folder\"'''\n",
    "    logger.info('Saved scraped data to csv')\n",
    "    scraped_date = str(datetime.datetime.now().date())\n",
    "    scraped_page = page_url.replace('/','_').replace(',','')\n",
    "    df.to_csv(f'daily_scrape_files/apartments{scraped_page}{scraped_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    logger.info('Started Scraping')    \n",
    "    \n",
    "    url_list = get_url_list(base_url, page_url)\n",
    "\n",
    "    apts_data = get_all_apartments(url_list)\n",
    "\n",
    "    df = create_df(apts_data)\n",
    "\n",
    "    df = df_formatter(df)\n",
    "\n",
    "    save_to_csv()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f'Finished.  Total program time: {round(elapsed_time/60,2)} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
