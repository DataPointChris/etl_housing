{
 "cells": [
  {
   "source": [
    "## https://realpython.com/modern-web-automation-with-python-and-selenium/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set path to current working directory for cron job\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "# formatter = logging.Formatter('%(asctime)s:%(name)s:%(message)s')\n",
    "# file_handler = logging.FileHandler('scraper.log')\n",
    "# file_handler.setFormatter(formatter)\n",
    "# logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# logger = logging.getLogger('my_logger')\n",
    "# handler = RotatingFileHandler('my_log.log', maxBytes=2000, backupCount=10)\n",
    "# logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIES = None\n",
    "SAVE_DIRECTORY = 'daily_scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityScraper:\n",
    "    \"\"\"Scrapes a city for apartment listings.\"\"\"\n",
    "    \n",
    "    def __init__(self, city, save_directory='scraped'):\n",
    "        self.city = city\n",
    "        self.city_url = f'https://www.trulia.com/for_rent/{city}'\n",
    "        self.browser = self._set_browser()\n",
    "\n",
    "    def _set_browser(self):\n",
    "        firefox_options = webdriver.FirefoxOptions()\n",
    "        firefox_options.set_headless()\n",
    "        browser = webdriver.Firefox(firefox_options=firefox_options)\n",
    "        return browser\n",
    "\n",
    "    def browser_get(self, url):\n",
    "        \"\"\" Gets the url specified.  Contains error handling\"\"\"\n",
    "        try:\n",
    "            self.browser.get(url)\n",
    "        except (ConnectionError, ConnectionResetError) as e:\n",
    "            logger.info(f'Error {e} for URL: {url}')\n",
    "\n",
    "    def get_list_page_urls(self):\n",
    "        \"\"\"Gets the urls on the list page\"\"\"\n",
    "        listing_urls = []\n",
    "        for elem in self.browser.find_elements_by_class_name('jLNYlr'):\n",
    "            try:\n",
    "                listing_urls.append(elem.find_element_by_tag_name('a').get_attribute('href'))\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        return listing_urls\n",
    "\n",
    "    def get_next_page(self):\n",
    "        \"\"\" Gets next page link and returns flag indicating if last page\"\"\"\n",
    "        try:\n",
    "            # next_page = self.browser.find_element_by_class_name('bpESQu').get_attribute('href')\n",
    "            soup = BeautifulSoup(self.browser.page_source, 'html.parser')\n",
    "            href_suffix = soup.find('a', {'aria-label': 'Next Page'})['href']\n",
    "            next_page = 'https://www.trulia.com' + href_suffix\n",
    "            last_page = False\n",
    "            print(f'HREF SUFFIX: {href_suffix}')\n",
    "            return next_page, last_page\n",
    "        except TypeError:\n",
    "            next_page = None\n",
    "            last_page = True\n",
    "            return next_page, last_page\n",
    "\n",
    "    def get_apartment_urls_for_city(self):\n",
    "        '''Gets a list of urls for city from all listing pages'''\n",
    "        i = 1\n",
    "        url_list = []\n",
    "        next_page = self.city_url\n",
    "        last_page = False\n",
    "        while last_page is False:\n",
    "        # for _ in range(1):\n",
    "            print(f'Page {i}, Total Apartment URLs: {len(url_list)}') if i % 10 == 0 else None\n",
    "            self.browser_get(next_page)\n",
    "\n",
    "            list_page_urls = self.get_list_page_urls()\n",
    "            url_list.extend(list_page_urls)\n",
    "            print(f'Current URL: {self.browser.current_url}')\n",
    "            print(f'Last page: {last_page}')\n",
    "            next_page, last_page = self.get_next_page()\n",
    "            print(f'NEW Next page: {next_page}')\n",
    "            print(f'NEW Last page: {last_page}')\n",
    "            print(f'Iteration: {i}')\n",
    "            i += 1\n",
    "            time.sleep(.05)\n",
    "        return url_list\n",
    "\n",
    "    def get_apartment_data(self, url):\n",
    "        '''Gets apartment data for the url specified'''\n",
    "        self.browser_get(url)\n",
    "        content = self.browser.page_source\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        apartment_list = []\n",
    "\n",
    "        for floor_plan_table in soup.find_all('table', {'data-testid': 'floor-plan-group'}):\n",
    "            for tr in floor_plan_table.find_all('tr'):\n",
    "\n",
    "                unit = tr.find('div', {'color': 'highlight'}).text\n",
    "\n",
    "                sqft = tr.find('td', {'class': 'FloorPlanTable__FloorPlanFloorSpaceCell-sc-1ghu3y7-5'}).text\n",
    "\n",
    "                bed = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[0].text\n",
    "\n",
    "                bath = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[1].text\n",
    "\n",
    "                price = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanCell-sc-1ghu3y7-2',\n",
    "                                        'class': 'FloorPlanTable__FloorPlanSMCell-sc-1ghu3y7-8'},\n",
    "                                    limit=2)[1].text\n",
    "\n",
    "                name = soup.find('span', {'data-testid': 'home-details-summary-headline'}).text\n",
    "\n",
    "                address = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[0].text\n",
    "\n",
    "                city_state_zip = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[1].text\n",
    "\n",
    "                city, state, zipcode = city_state_zip.replace(',', '').rsplit(maxsplit=2)\n",
    "\n",
    "                description = soup.find('div', {'data-testid': 'home-description-text-description-text'}).text\n",
    "\n",
    "                details = [detail.text for detail in soup.find_all(\n",
    "                    'li', {'class': 'FeatureList__FeatureListItem-iipbki-0'}\n",
    "                )]\n",
    "                details = ' ,'.join(details)\n",
    "\n",
    "                apartment_url = url\n",
    "                date = str(datetime.datetime.now().date())\n",
    "                apartment_list.append([name, address, unit, sqft, bed, bath, price, city,\n",
    "                                    state, zipcode, description, details, apartment_url, date])\n",
    "        return apartment_list\n",
    "\n",
    "    def create_apartment_df(self, data):\n",
    "        df = pd.DataFrame(data, columns=['name', 'address', 'unit', 'sqft', 'bed', 'bath', 'price',\n",
    "                               'city', 'state', 'zipcode', 'description', 'details', 'url', 'date'])\n",
    "        return df\n",
    "\n",
    "    def clean_apartment_df(self, df):\n",
    "        '''Formats the dataframe to remove special characters, spaces, and NaN values.\n",
    "        Removes units with price ranges, rather than one specified price.'''\n",
    "\n",
    "        df.sqft = df.sqft.str.replace('sqft', '').str.replace(',', '').str.strip()\n",
    "        df = df.loc[df.sqft != ''] # throw out empty square foot apts\n",
    "        mask = df.sqft.str.contains('-')\n",
    "        df.loc[mask, 'sqft'] = df.loc[mask, 'sqft'].apply(lambda x: np.mean(list(map(int, x.split('-')))))\n",
    "        df.price = df.price.str.replace('Contact', '')\n",
    "        df.price = df.price.str.replace('$', '').str.replace(',', '').str.replace('+', '').str.strip()\n",
    "        df.bath = df.bath.str.replace('ba', '').str.strip()\n",
    "        df.bed = df.bed.str.replace('bd', '').str.lower().replace('studio', 0).str.strip()\n",
    "        df.bed = df.bed.replace(np.nan, 0)\n",
    "        df = df[~df.price.str.contains('-', na=False)]  # throw out price range apts\n",
    "        df.replace(' ', '', inplace=True)  # whitespace to blank\n",
    "        df.replace('', np.nan, inplace=True)  # blank to NaN\n",
    "        # df.dropna(inplace=True)  # drop NaN rows\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def convert_df_columns(self, df):\n",
    "        '''Converts rows to numeric and float for calculations'''\n",
    "        df = df.astype({'sqft': 'int32', 'price': 'int32', 'bath': 'float32', \n",
    "                        'bed': 'float32', 'zipcode': 'int32'})\n",
    "        return df\n",
    "\n",
    "    def save_to_csv(self, df):\n",
    "        '''Saves cleaned dataframe to csv file in \"daily_scrape_files\" folder\"'''\n",
    "        scraped_date = str(datetime.datetime.now().date())\n",
    "        city_name = self.city.replace(',', '_')\n",
    "        filedir = f'{SAVE_DIRECTORY}/{city_name}'\n",
    "        os.makedirs(filedir, exist_ok=True)\n",
    "        df.to_csv(f'{filedir}/{scraped_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START scraping Woburn,MA\n",
      "Getting URL list\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/\n",
      "Last page: False\n",
      "HREF SUFFIX: /for_rent/Woburn,MA/2_p/\n",
      "NEW Next page: https://www.trulia.com/for_rent/Woburn,MA/2_p/\n",
      "NEW Last page: False\n",
      "Iteration: 1\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/2_p/\n",
      "Last page: False\n",
      "HREF SUFFIX: /for_rent/Woburn,MA/3_p/\n",
      "NEW Next page: https://www.trulia.com/for_rent/Woburn,MA/3_p/\n",
      "NEW Last page: False\n",
      "Iteration: 2\n",
      "Current URL: https://www.trulia.com/for_rent/Woburn,MA/3_p/\n",
      "Last page: False\n",
      "NEW Next page: None\n",
      "NEW Last page: True\n",
      "Iteration: 3\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]URLs retrieved: 92\n",
      "Getting apartment data from url_list\n",
      "100%|██████████| 92/92 [01:37<00:00,  1.06s/it]Apartments retrieved: 175\n",
      "FINISH scraping Woburn,MA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities = ['Woburn,MA']\n",
    "\n",
    "for city in cities:\n",
    "    print(f'START scraping {city}')    \n",
    "    scraper = CityScraper(city, save_directory=SAVE_DIRECTORY)\n",
    "    \n",
    "    print('Getting URL list')\n",
    "    apartment_url_list = scraper.get_apartment_urls_for_city()\n",
    "    print(f'URLs retrieved: {len(apartment_url_list)}')\n",
    "    \n",
    "    print('Getting apartment data from url_list')\n",
    "    apartments_data = []\n",
    "    for i, apartment_url in enumerate(tqdm(apartment_url_list), start=1):\n",
    "        print(f'URL {i} of {len(apartment_url_list)}') if i % 500 == 0 else None\n",
    "        try:\n",
    "            apt_data = scraper.get_apartment_data(apartment_url)\n",
    "            apartments_data.extend(apt_data)\n",
    "        except Exception as e:\n",
    "            print(f'Exception getting apartment data for url {apartment_url}', e)\n",
    "            # logger.exception('Error adding data to list', e)\n",
    "            continue\n",
    "    print(f'Apartments retrieved: {len(apartments_data)}')\n",
    "    \n",
    "    apartment_df = scraper.create_apartment_df(apartments_data)\n",
    "    cleaned_apartment_df = scraper.clean_apartment_df(apartment_df)\n",
    "    cleaned_and_converted_apartment_df = scraper.convert_df_columns(cleaned_apartment_df)\n",
    "    scraper.save_to_csv(cleaned_and_converted_apartment_df)\n",
    "    print(f'FINISH scraping {city}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aria-label=\"Next Page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = CityScraper('Cambridge,MA')\n",
    "\n",
    "\n",
    "# cleaned = scc.clean_apartment_df(apartment_df)\n",
    "# cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.browser.get('https://www.trulia.com/for_rent/Woburn,MA/2_p/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(scc.browser.page_source, 'html.parser')\n",
    "next_page = soup.find('a', {'aria-label': 'Next Page'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/for_rent/Woburn,MA/3_p/'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "soup.find('a', {'aria-label': 'Next Page'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.trulia.com/for_rent/Woburn,MA/2_p/'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "scc.browser.current_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned.astype({'sqft': 'int32', 'price': 'int32', 'bath': 'float32', 'bed': 'float32',\n",
    "#                 'zipcode': 'int32'}).dtypes\n",
    "# cleaned[cleaned['price'].apply(lambda x: not x.isnumeric())]\n",
    "converted = scc.convert_df_columns(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.save_to_csv(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1      2020-12-24\n",
       "2      2020-12-24\n",
       "4      2020-12-24\n",
       "5      2020-12-24\n",
       "6      2020-12-24\n",
       "          ...    \n",
       "686    2020-12-24\n",
       "687    2020-12-24\n",
       "688    2020-12-24\n",
       "689    2020-12-24\n",
       "690    2020-12-24\n",
       "Name: date, Length: 583, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "cleaned.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "@function_timer\n",
    "def main():    \n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Started Scraping\n",
      "Getting URL list\n",
      "0it [00:00, ?it/s]URLs retrieved: 0\n",
      "Getting apartment data from url_list\n",
      "Elapsed time: 0.0 minutes for <function get_all_apartments at 0x11d17faf0>\n",
      "Apartments retrieved: 0\n",
      "Creating DataFrame from apartment list\n",
      "Elapsed time: 0.0 minutes for <function create_df at 0x11d17fc10>\n",
      "Formatting Dataframe\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'(Series([], Name: sqft, dtype: bool), 'sqft')' is an invalid key",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-69d9a74da3d2>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Elapsed time: {round(elapsed_time/60,2)} minutes for {func}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-69d9a74da3d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Formatting Dataframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mdf_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Converting Dataframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-69d9a74da3d2>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Elapsed time: {round(elapsed_time/60,2)} minutes for {func}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-69d9a74da3d2>\u001b[0m in \u001b[0;36mdf_formatter\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sqft'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sqft'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sqft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Contact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/etl_housing-lSJN5Kdt/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/etl_housing-lSJN5Kdt/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0mcasted_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(Series([], Name: sqft, dtype: bool), 'sqft')' is an invalid key"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}