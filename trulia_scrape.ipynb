{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set path to current working directory for cron job\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "# formatter = logging.Formatter('%(asctime)s:%(name)s:%(message)s')\n",
    "# file_handler = logging.FileHandler('scraper.log')\n",
    "# file_handler.setFormatter(formatter)\n",
    "# logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# logger = logging.getLogger('my_logger')\n",
    "# handler = RotatingFileHandler('my_log.log', maxBytes=2000, backupCount=10)\n",
    "# logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "           'accept-encoding': 'gzip, deflate, sdch, br',\n",
    "           'accept-language': 'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
    "           'cache-control': 'max-age=0',\n",
    "           'upgrade-insecure-requests': '1',\n",
    "           'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}\n",
    "\n",
    "base_url = 'https://www.trulia.com'\n",
    "page_url = '/for_rent/Austin,TX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        return_value = func(*args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Elapsed time: {round(elapsed_time/60,2)} minutes for {func}')\n",
    "        return return_value\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def get_url_list(base_url, page_url):\n",
    "    '''Gets a list of urls from main page to scrape.'''\n",
    "    url_list = []\n",
    "    last_page = False\n",
    "    while last_page is False:\n",
    "        try:\n",
    "            response = requests.get(base_url + page_url, headers=headers)\n",
    "        except (ConnectionError, ConnectionResetError) as e:\n",
    "            logger.info(f'Error {e}')\n",
    "            continue\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.info(\"Failed: \", response.status_code)\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        for div in soup.find_all('div', {'data-hero-element-id': 'srp-home-card', 'data-hero-element-id': 'false'}):\n",
    "            url = div.find('a').attrs['href']\n",
    "            url_list.append(url)\n",
    "\n",
    "        # check if last page and exit while loop\n",
    "        if soup.find('a', {'aria-label': 'Next Page'}):\n",
    "            last_page = False\n",
    "            page_url = soup.find('a', {'aria-label': 'Next Page'})['href']\n",
    "            sleep(.1)\n",
    "        else:\n",
    "\n",
    "            last_page = True\n",
    "\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "def get_apartment_data(base_url, current_url):\n",
    "    '''Gets apartment data for the url specified'''\n",
    "    try:\n",
    "        response = requests.get(base_url + current_url, headers=headers)\n",
    "    except (ConnectionError, ConnectionResetError) as e:\n",
    "        print(f'Error {e}')\n",
    "    if response.status_code != 200:\n",
    "        print(f'Failed: {response.status_code}')\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    apartment_list = []\n",
    "\n",
    "    for floor_plan_table in soup.find_all('table', {'data-testid': 'floor-plan-group'}):\n",
    "        for tr in floor_plan_table.find_all('tr'):\n",
    "\n",
    "            unit = tr.find('div', {'color': 'highlight'}).text\n",
    "\n",
    "            sqft = tr.find('td', {'class': 'FloorPlanTable__FloorPlanFloorSpaceCell-sc-1ghu3y7-5'}).text\n",
    "\n",
    "            bed = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[0].text\n",
    "\n",
    "            bath = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanFeaturesCell-sc-1ghu3y7-4'})[1].text\n",
    "\n",
    "            price = tr.find_all('td', {'class': 'FloorPlanTable__FloorPlanCell-sc-1ghu3y7-2',\n",
    "                                       'class': 'FloorPlanTable__FloorPlanSMCell-sc-1ghu3y7-8'},\n",
    "                                limit=2)[1].text\n",
    "\n",
    "            name = soup.find('span', {'data-testid': 'home-details-summary-headline'}).text\n",
    "\n",
    "            address = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[0].text\n",
    "\n",
    "            city_state_zip = soup.find_all('span', {'data-testid': 'home-details-summary-city-state'})[1].text\n",
    "\n",
    "            city, state, zipcode = city_state_zip.replace(',', '').rsplit(maxsplit=2)\n",
    "\n",
    "            description = soup.find('div', {'data-testid': 'home-description-text-description-text'}).text\n",
    "\n",
    "            details = [detail.text for detail in soup.find_all(\n",
    "                'li', {'class': 'FeatureList__FeatureListItem-iipbki-0 dArMue'}\n",
    "            )]\n",
    "            details = ' ,'.join(details)\n",
    "\n",
    "            apartment_url = base_url + current_url\n",
    "            date = str(datetime.datetime.now().date())\n",
    "            apartment_list.append([name, address, unit, sqft, bed, bath, price, city,\n",
    "                                   state, zipcode, description, details, apartment_url, date])\n",
    "    return apartment_list\n",
    "\n",
    "\n",
    "@function_timer\n",
    "def get_all_apartments(url_list):\n",
    "    '''Wrapper function using \"get_apartment_data\" function to get data for all apartments in \"url_list\"'''\n",
    "    apts_data = []\n",
    "    for i, current_url in enumerate(tqdm(url_list), start=1):\n",
    "        if i % 500 == 0:\n",
    "            print(f'URL {i} of {len(url_list)}')\n",
    "        sleep(.05)\n",
    "        try:\n",
    "            apts_data.extend(get_apartment_data(base_url, current_url))\n",
    "        except Exception as e:\n",
    "            print('Exception', e)\n",
    "#             logger.exception('Error adding data to list', e)\n",
    "            continue\n",
    "    \n",
    "    return apts_data\n",
    "\n",
    "\n",
    "@function_timer\n",
    "def create_df(data):\n",
    "    df = pd.DataFrame(data,\n",
    "                      columns=['name', 'address', 'unit', 'sqft', 'bed', 'bath', 'price',\n",
    "                               'city', 'state', 'zipcode', 'description', 'details', 'url', 'date'])\n",
    "    return df\n",
    "\n",
    "\n",
    "@function_timer\n",
    "def df_formatter(df):\n",
    "    '''Formats the dataframe to remove special characters, spaces, and NaN values.\n",
    "       Removes units with price ranges, rather than one specified price.'''\n",
    "\n",
    "    df.sqft = df.sqft.str.replace('sqft', '').str.replace(',', '').str.strip()\n",
    "    mask = df.sqft.str.contains('-')\n",
    "    df.loc[mask, 'sqft'] = df[mask, 'sqft'].apply(lambda x: np.mean(list(map(int, x.split('-')))))\n",
    "    df.price = df.price.str.replace('Contact', '')\n",
    "    df.price = df.price.str.replace('$', '').str.replace(',', '').str.strip()\n",
    "    df.bath = df.bath.str.replace('ba', '').str.strip()\n",
    "    df.bed = df.bed.str.replace('bd', '').str.lower().replace('studio', 0).str.strip()\n",
    "    df.bed = df.bed.replace(np.nan, 0)\n",
    "    df = df[~df.price.str.contains('-', na=False)]\n",
    "    df.replace(' ', '', inplace=True)  # whitespace to blank\n",
    "    df.replace('', np.nan, inplace=True)  # blank to NaN\n",
    "    df.dropna(inplace=True)  # drop NaN rows\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "@function_timer\n",
    "def df_converter(df):\n",
    "    '''Converts rows to numeric and float for calculations'''\n",
    "    df = df.astype({'sqft': 'int32', 'price': 'int32', 'bath': 'float32', 'bed': 'float32'})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@function_timer\n",
    "def save_to_csv(df):\n",
    "    '''Saves cleaned dataframe to csv file in \"daily_scrape_files\" folder\"'''\n",
    "    scraped_date = str(datetime.datetime.now().date())\n",
    "    scraped_page = page_url.replace('/', '_').replace(',', '')\n",
    "    df.to_csv(f'daily_scrape_files/apartments{scraped_page}{scraped_date}.csv', index=False)\n",
    "    \n",
    "\n",
    "@function_timer\n",
    "def main():    \n",
    "    print('Started Scraping')\n",
    "    \n",
    "    print('Getting URL list')\n",
    "    url_list = get_url_list(base_url, page_url)\n",
    "    print(f'URLs retrieved: {len(url_list)}')\n",
    "    \n",
    "    print('Getting apartment data from url_list')\n",
    "    apts_data = get_all_apartments(url_list)\n",
    "    print(f'Apartments retrieved: {len(apts_data)}')\n",
    "    \n",
    "    print('Creating DataFrame from apartment list')\n",
    "    df = create_df(apts_data)\n",
    "    \n",
    "    print('Formatting Dataframe')\n",
    "    df_fmt = df_formatter(df)\n",
    "    \n",
    "    print('Converting Dataframe')\n",
    "    df_cvt = df_converter(df_fmt)\n",
    "    \n",
    "    print('Saving scraped data to csv')\n",
    "    save_to_csv(df_cvt)\n",
    "        \n",
    "    print('Finished scraping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
